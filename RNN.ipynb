{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.10.2+cu102  Device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:6')\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_datasets = datasets.CIFAR10(root='../data/CIFAR_10', train=True, download=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))\n",
    "test_datasets = datasets.CIFAR10(root='../data/CIFAR_10', train=False, transform=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))\n",
    "train_loader = torch.utils.data.DataLoader(dataset= train_datasets, batch_size=BATCHSIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_datasets, batch_size=BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size= 3, stride = stride,padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size= 3, stride = 1,padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias= False), nn.BatchNorm2d(planes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_Classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride = 1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride = 2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride = 2)\n",
    "        self.linear = nn.Linear(64, num_Classes)\n",
    "    \n",
    "    def _make_layer(self,planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= (len(test_loader.dataset)/BATCHSIZE)\n",
    "    test_accuracy = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.435867\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.623470\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.612515\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.051703\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.695855\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.090916\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.079330\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.202735\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.0356, \tTest Accuracy: 63.24 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 0.881009\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 0.810281\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 0.664034\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 0.850634\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 0.773943\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 0.899879\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 0.712756\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 0.744185\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.8723, \tTest Accuracy: 69.10 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 0.975473\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 0.653662\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 0.726770\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 0.710255\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 0.441928\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 0.653439\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 0.731788\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 0.893008\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.7393, \tTest Accuracy: 73.64 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 0.358365\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 0.714162\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 0.833280\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 0.633489\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 0.825878\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 0.790111\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 0.356999\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 0.483543\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.6859, \tTest Accuracy: 75.81 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 1.048479\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.467169\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 0.452906\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.901315\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 0.461984\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 0.386089\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 0.492238\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 0.479523\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.6878, \tTest Accuracy: 76.58 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 0.508440\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 0.562954\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 0.551283\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.826926\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 0.276427\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.443384\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 0.634623\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 0.559442\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.6204, \tTest Accuracy: 78.72 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 0.696948\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 0.456269\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.618450\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.527920\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 0.468622\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.283449\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 0.304082\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.331413\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.5752, \tTest Accuracy: 79.84 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 0.516685\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 0.414185\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 0.436903\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 0.410272\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.302914\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.667197\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 0.497345\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.251242\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.5424, \tTest Accuracy: 81.22 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.531775\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 0.400176\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.436359\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.335502\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 0.783375\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.422317\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.426805\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 0.442183\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.5433, \tTest Accuracy: 81.82 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.262907\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.385466\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.467672\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.499889\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.300935\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 0.159041\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.502218\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.574232\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.5154, \tTest Accuracy: 82.33 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d1aa92d2b669f5ff4ea8794858f459d16fcce2139883dca779c71d99d2e4135"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
